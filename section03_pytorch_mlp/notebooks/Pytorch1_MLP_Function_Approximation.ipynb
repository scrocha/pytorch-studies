{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> Approximating a Region of the Sine Function </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will approximate a region of the sine function with a neural network to get a sense of how architecture and hyperparameters affect neural network performance.\n",
    "\n",
    "<img src=\"../data/sine_wave.gif\" width=\"1200\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Pytorch Datasets and Dataloaders </h2>\n",
    "<b> *Basics we will cover more detail later on, but for now...</b><br>\n",
    "Pytorch has a huge number of functionalities that make training our neural networks very easy! One of those functionalities is the Pytorch dataset and dataloader (they are real life-savers!). The \"dataset\" class is an object that \"stores\" our dataset either directly (it loads all the data in the initialisation function) or indirectly (it loads the image paths during the initialisation function and only loads them when it needs to - for large image-based datasets this is usually the only way to do it).We will see how we can create our own Pytorch dataset soon!<br>\n",
    "These datasets then are used to create a \"dataloader\" object that is \"iterable\". The Pytorch dataloader will take our dataset and randomly shuffle it (if we tell it to), it will also divide the dataset into \"mini-batches\" which are groups of datapoints of a fixed size (the batch size). Our Neural Network is then trained through a single step of GD on this mini-batch. As we iterate through the dataloader, the dataloader will pass us a new unique mini-batch until the whole dataset has been passed to us. One whole loop through the dataset is called an \"epoch\", during every epoch the dataset is re-shuffled so the mini-batches are all random. This random sampling of the dataset and training on mini-batches (instead of performing GD on the whole dataset) is called Stochastic Gradient Descent (SGD)<br>\n",
    "Note: If the whole dataset does not evenly divide into mini batches then in the last iterator we will just be passed whatever is left over!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Creating a Pytorch dataset </h3>\n",
    "The dataset we will be creating will be points from a \"noisey\" sine wave.<br>\n",
    "The Pytorch dataset class has three essential parts:<br>\n",
    "The __init__ function (as most Python classes do)<br>\n",
    "The __getitem__ function (this is called during every iteration)<br>\n",
    "The __len__ function (this must return the length of the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"SineDataset\" class by inheriting the Pytorch Dataset class\n",
    "class SineDataset(Dataset):\n",
    "    \"\"\" Data noisey sinewave dataset\n",
    "        num_datapoints - the number of datapoints you want\n",
    "    \"\"\"\n",
    "    def __init__(self, num_datapoints):\n",
    "        # Lets generate the noisy sinewave points\n",
    "        \n",
    "        # Create \"num_datapoints\" worth of random x points using a uniform distribution (0-1) using torch.rand\n",
    "        # Then scale and shift the points to be between -9 and 9\n",
    "        self.x_data = # TO DO\n",
    "        \n",
    "        # Calculate the sin of all data points in the x vector and the scale amplitude down by 2.5\n",
    "        self.y_data = # TO DO\n",
    "        \n",
    "        # Add some gaussein noise to each datapoint using torch.randn_like \n",
    "        # (scale the noise down a bit by about 20 - see how different noise levels effects your model)\n",
    "        # Note:torch.randn_like will generate a tensor of gaussein noise the same size \n",
    "        # and type as the provided tensor\n",
    "        self.y_data += # TO DO\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # This function is called by the dataLOADER class whenever it wants a new mini-batch\n",
    "        # The dataLOADER class will pass the dataSET class a number of datapoint indexes (mini-batch of indexs)\n",
    "        # It is up to the dataSET's __getitem__ function to output the corresponding input datapoints \n",
    "        # AND the corresponding labels\n",
    "        return # TO DO\n",
    "    \n",
    "        # Note:Pytorch will actually pass the __getitem__ function one index at a time\n",
    "        # If you use multiple dataLOADER \"workers\" multiple __getitem__ calls will be made in parallel\n",
    "        # (Pytorch will spawn multiple threads)\n",
    "\n",
    "    def __len__(self):\n",
    "        # We also need to specify a \"length\" function, Python will use this fuction whenever\n",
    "        # You use the Python len(function)\n",
    "        # We need to define it so the dataLOADER knows how big the dataSET is!\n",
    "        return # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our dataset, lets create an instance of it for training and testing and then create  dataloaders to make it easy to iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x_train = 30000   # the number of training datapoints\n",
    "n_x_test = 8000     # the number of testing datapoints\n",
    "batch_size = 16\n",
    "\n",
    "# Create an instance of the SineDataset for both the training and test set\n",
    "dataset_train = # TO DO\n",
    "dataset_test  = # TO DO\n",
    "\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "# Now we need to pass the dataSET to the Pytorch dataLOADER class along with some other arguments\n",
    "# dataset - the dataset for this dataloader\n",
    "# batch_size - the size of our mini-batches\n",
    "# shuffle - whether or not we want to shuffle the dataset (True for training False for testing)\n",
    "    \n",
    "data_loader_train = # TO DO\n",
    "data_loader_test = # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualise the dataset we've created!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "plt.scatter(dataset_train.x_data, dataset_train.y_data, s=0.2)\n",
    "# Note:see here how we can just directly access the data from the dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Neural Network Architecture</h2>\n",
    "<b> Non-Linear function approximators! </b> <br>\n",
    "\n",
    "Up until now we have only created a single linear layer with an input layer and an output layer. In this section we will start to create multi-layered networks with many \"hidden\" layers separated by \"activation functions\" that give our networks \"non-linearities\". If we didn't have these activation functions and simply stacked layers together, our network would be no better than a single linear layer! Why? Because multiple sequential \"linear transformations\" can be modeled with just a single linear transformation. This is easiest to understand with matrix multiplications (which is exactly what happens inside a linear layer).<br>\n",
    "\n",
    "$M_o = M_i*M_1*M_2*M_3*M_4*M_5$<br>\n",
    "Is the same as<br>\n",
    "$M_o = M_i*M_T$<br>\n",
    "Where<br>\n",
    "$M_T = M_1*M_2*M_3*M_4*M_5$<br>\n",
    "\n",
    "Aka multiplication with several matrices can be simplified to multiplication with a single matrix.<br>\n",
    "\n",
    "So what are these nonlinear activation functions that turn our simple linear models into a power \"nonlinear function approximator\"? Some common examples are:<br>\n",
    "1. relu\n",
    "2. sigmoid\n",
    "3. tanh\n",
    "\n",
    "Simply put they are \"nonlinear\" functions, the simplest of which is the \"rectified linear unit\" (relu) which is \"piecewise non-linear\".\n",
    "\n",
    "NOTE: The term \"layer\" most commonly refers to the inputs or outputs of the weight matrix or activations functions and not the linear layer or activation layer themselves. Output layers in between two \"linear layers\" are called \"hidden layers\". You can imagine them \"inside\" the neural network with us only being able to see the input and output layers. To confuse things even further the outputs of activation functions are also commonly called \"activations\"\n",
    "\n",
    "Why do we want a linear function approximator? Because many processes, tasks, systems in the real world are non-linear. \"Linear\" in basic terms refers to any process that takes inputs, scales them and sums them together to get an output. \n",
    "\n",
    "\n",
    "## Regression or Classification Neural Networks do only one thing....\n",
    "In this notebook we are performing regression, which as we've seen is very similar to classification! Both regression and classification can be thought of as producing a distribution over possible values for a given input. In classification the model produces the probability that the input belongs to a particular category where the probabilities define a discrete categorical distribution (or a Bernoulli distribution for binary classification)!\n",
    "In regression our model also produces a distribution on the output, however it may be less clear how. The output of the model is in fact the expectation (mean) of a normal distribution with sigma (standard deviation) equal to 1 (assuming we are using the basic MSE loss). In fact the Mean Squared Error (MSE) loss we use can be thought of in the same way as the cross entropy loss used in classification! With the MSE loss we are trying to learn a model that produces a normal distribution (conditioned on the input data) such that the target value has the highest likelihood! Where does the normal distribution have the highest likelihood? At the mean!!<br>\n",
    "For more information as to how we get the MSE loss from the Maximum likelihood of a normal distribution have a look at the following:<br>\n",
    "[Blog: MSE is Cross Entropy at heart by Moein Shariatnia](https://towardsdatascience.com/mse-is-cross-entropy-at-heart-maximum-likelihood-estimation-explained-181a29450a0b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pytorch nn.Module</h3>\n",
    "Now we can define a Pytorch model to be trained!<br>\n",
    "To do so we use the Pytorch nn.Module class as the base for defining our network. Just like the dataset class, this class has a number of important functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our network class by using the nn.module\n",
    "class ShallowLinear(nn.Module):\n",
    "    '''\n",
    "    A simple, general purpose, fully connected network\n",
    "    '''\n",
    "    # Here we initialise our network and define all the layers we need\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        # Perform initialization of the pytorch superclass, this will allow us to inherit \n",
    "        # functions from the nn.Module class\n",
    "        super(ShallowLinear, self).__init__()\n",
    "        \n",
    "        # Define 4 linear layers for our model with the same hidden size for all\n",
    "        # Note: the output of one layer must be the same size as the input to the next!\n",
    "        # TO DO\n",
    "        # TO DO\n",
    "        # TO DO\n",
    "        # TO DO\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This function is an important one and we must create it or pytorch will give us an error!\n",
    "        # This function defines the \"forward pass\" of our neural network\n",
    "        # and will be called when we simply call our network class\n",
    "        # aka we can do net(input) instead of net.forward(input)\n",
    "        \n",
    "        # Lets define the sqeuence of events for our forward pass!\n",
    "        # We'll use a tanh activation function here\n",
    "        # You can experiment with other activation functions!\n",
    "        x = # TO DO layer 1\n",
    "        x = # TO DO activation function\n",
    "        \n",
    "        x = # TO DO layer 2\n",
    "        x = # TO DO activation function\n",
    "        \n",
    "        x = # TO DO layer 3\n",
    "        x = # TO DO activation function\n",
    "\n",
    "        # No activation function on the output!!\n",
    "        x = # TO DO layer 4\n",
    "        \n",
    "        # Note we re-use the variable x as we don't care about overwriting it \n",
    "        # though in later labs we will want to use earlier hidden layers\n",
    "        # later in our network!\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters,  model and optimizer\n",
    "\n",
    "Here we define the following parameters for training:\n",
    "\n",
    "- batch size (which has already been defined)\n",
    "- learning rate\n",
    "- number of training epochs\n",
    "- optimizer\n",
    "- loss function\n",
    "\n",
    "Ideally, numeric parameters would be tested empirically with an exhaustive search. When testing manually, It is recommended to maximize the model fit with one parameter at a time to avoid confounding your results. \n",
    "\n",
    "Try these learning rates:\n",
    "- 5e-2, 1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 5e-5\n",
    "\n",
    "Try these optimizers:\n",
    "- `optim.SGD(shallow_model.parameters(), lr=learning_rate)`\n",
    "- `optim.Adam(shallow_model.parameters(), lr=learning_rate)`\n",
    "\n",
    "[Youtube: Optimizers - EXPLAINED! by CodeEmporium](https://youtu.be/mdKjMPmcWjY?si=VtjuF_QlPHAzj2Sx)\n",
    "\n",
    "See the pytorch documentation pages for an extensive list of options:\n",
    "- Optimizers: http://pytorch.org/docs/master/optim.html#algorithms\n",
    "- Loss: http://pytorch.org/docs/master/nn.html#id46\n",
    "\n",
    "Read this page for a detailed comparison of optimizers: http://ruder.io/optimizing-gradient-descent/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "learning_rate = # TO DO\n",
    "nepochs = # TO DO\n",
    "\n",
    "# Create model\n",
    "shallow_model = # TO DO\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = # TO DO\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = # TO DO mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate training, plot testing results\n",
    "Here we put all the previous methods together to train and test the model. This problem is an unusual one in that our loss is the best quantitative metric of the model performance. Classification problems require further analysis of true/false positives/negatives.\n",
    "\n",
    "Rerun this cell several times without editing any parameters. Is the result the same?\n",
    "\n",
    "Try a larger batch size, how is the training time affected?\n",
    "\n",
    "Look at the slope and noise level of the loss plot. Does it look like the training converged on a local minimum?\n",
    "\n",
    "Try some different hyperparameters and see how accurate you can get your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create two lists to store the loss values from training and testing\n",
    "training_loss_logger = []\n",
    "testing_loss_logger = []\n",
    "# Note:create them outside of the train/test cell so they don't get overwritten \n",
    "# if we want to run the cell again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main train/test cell\n",
    "# This will run one epoch of training and then one epoch of testing nepochs times\n",
    "for epoch in trange(nepochs, desc=\"Epochs\", leave=False):\n",
    "    \n",
    "    # Perform training Loop!\n",
    "    for x, y in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "\n",
    "        # Run forward calculation\n",
    "        # TO DO\n",
    "        \n",
    "        # Compute loss.\n",
    "        # TO DO\n",
    "        \n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable weights\n",
    "        # of the model)\n",
    "        # TO DO\n",
    "        \n",
    "        # Backward pass: compute gradient of the loss with respect to model\n",
    "        # parameters\n",
    "        # TO DO\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        # TO DO\n",
    "\n",
    "        # Log the loss so we can visualise the training plot later\n",
    "        # TO DO\n",
    "\n",
    "    # Perform a test Loop!\n",
    "    # While we are within a \"with torch.no_grad():\" block, Pytorch will not construct the computational graph\n",
    "    # This helps speed up computation and saves memory while we are not training\n",
    "    with torch.no_grad():\n",
    "        # Create a variable to accumulate the test loss so we can take the average\n",
    "        test_loss_accum = 0\n",
    "        for i, (x, y) in enumerate(tqdm(data_loader_test, desc=\"Testing\", leave=False)):\n",
    "\n",
    "            # Run forward calculation\n",
    "            # TO DO\n",
    "            \n",
    "            # Compute loss.\n",
    "            # TO DO\n",
    "            \n",
    "            # Log the loss so we can visualise the training plot later\n",
    "            # TO DO\n",
    "            test_loss_accum += loss\n",
    "            \n",
    "        test_loss_accum /= (i + 1)\n",
    "        \n",
    "print(\"Epoch [%d/%d], Average Test Loss %.4f\" %(epoch, nepochs, test_loss_accum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lets visualise our results!</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out the test and train losses!\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one last epoch over the testing set, this time logging the models outputs!\n",
    "# Perform a test Loop!\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the testdata against the model's predicted outputs, how accurate does it look??\n",
    "# TO DO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
