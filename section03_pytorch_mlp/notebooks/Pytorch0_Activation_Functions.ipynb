{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Activation Functions for Non-Linear Function Approximation</h1> <br>\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/42/ReLU_and_GELU.svg\" width=\"500\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data distribution\n",
    "def create_data(data_points):\n",
    "    data_class1 = torch.rand(data_points, 2) * 1.45\n",
    "    data_class2 = torch.rand(data_points, 2) * 3\n",
    "    \n",
    "    mask = ~((data_class2[:, 0] < 1.55) * (data_class2[:, 1] < 1.55))\n",
    "    data_class2 = data_class2[mask]\n",
    "\n",
    "    # Lables\n",
    "    data_label1 = torch.zeros(data_class1.shape[0], 1)\n",
    "    data_label2 = torch.ones(data_class2.shape[0], 1)\n",
    "\n",
    "    # Combine data\n",
    "    x_data = torch.cat((data_class1, data_class2), 0)\n",
    "    y_data = torch.cat((data_label1, data_label2), 0)\n",
    "    \n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data\n",
    "x_train, y_train = create_data(1000)\n",
    "x_test, y_test = create_data(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Let's plot our data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see what the data looks like\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(x_train[:, 0].numpy(), x_train[:, 1].numpy(), c=y_train.flatten().numpy())\n",
    "_ = plt.xlabel(\"x0\")\n",
    "_ = plt.ylabel(\"x1\")\n",
    "_ = plt.title(\"Data Ground Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create a Linear Logistic Regression Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_linear = nn.Linear(2, 1) \n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create our optimizer - lr = 0.1\n",
    "logistic_optimizer = torch.optim.Adam(logistic_linear.parameters(), lr=0.1)\n",
    "# Number of times we iterate over the dataset\n",
    "max_epoch = 100\n",
    "\n",
    "logistic_loss_log = [] # keep track of the loss values\n",
    "logistic_acc = [] # keep track of the accuracy \n",
    "for epoch in range(max_epoch):\n",
    "    with torch.no_grad():\n",
    "        y_test_hat = logistic_linear(x_test)\n",
    "        \n",
    "        # The descision boundary is at 0.5 (between 0 and 1) AFTER the sigmoid\n",
    "        # The input to the Sigmoid function that gives 0.5 is 0!\n",
    "        # Therefore the descision boundary for the RAW output is at 0!!\n",
    "        class_pred = (y_test_hat > 0).float()\n",
    "        logistic_acc.append(float(sum(class_pred == y_test))/ float(y_test.shape[0]))\n",
    "        \n",
    "    # Perform a training step\n",
    "    y_train_hat = logistic_linear(x_train)\n",
    "    loss = loss_function(y_train_hat, y_train)\n",
    "    \n",
    "    logistic_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    logistic_optimizer.step()\n",
    "\n",
    "    logistic_loss_log.append(loss.item())\n",
    "    \n",
    "print(\"Accuracy of linear model(GD): %.2f%% \" %(logistic_acc[-1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see what the data looks like\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(x_test[:, 0].numpy(), x_test[:, 1].numpy(), c=class_pred.flatten().numpy())\n",
    "_ = plt.xlabel(\"x0\")\n",
    "_ = plt.ylabel(\"x1\")\n",
    "_ = plt.title(\"Linear Logistic Regression Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Non-Linear function approximators! </h2> <br>\n",
    "\n",
    "Up until now we have only created a single linear layer with an input layer and an output layer. In this section we will start to create multi-layered networks with many \"hidden\" layers separated by \"activation functions\" that give our networks \"non-linearities\". If we didn't have these activation functions and simply stacked layers together, our network would be no better than a single linear layer! Why? Because multiple sequential \"linear transformations\" can be modeled with just a single linear transformation. This is easiest to understand with matrix multiplications (which is exactly what happens inside a linear layer).<br>\n",
    "\n",
    "$M_o = M_i*M_1*M_2*M_3*M_4*M_5$<br>\n",
    "Is the same as<br>\n",
    "$M_o = M_i*M_T$<br>\n",
    "Where<br>\n",
    "$M_T = M_1*M_2*M_3*M_4*M_5$<br>\n",
    "\n",
    "Aka multiplication with several matrices can be simplified to multiplication with a single matrix.<br>\n",
    "\n",
    "So what are these nonlinear activation functions that turn our simple linear models into a power \"nonlinear function approximator\"? Some common examples are:<br>\n",
    "1. relu\n",
    "2. sigmoid\n",
    "3. tanh\n",
    "\n",
    "Simply put they are \"nonlinear\" functions, the simplest of which is the \"rectified linear unit\" (relu) which is \"piecewise non-linear\".\n",
    "\n",
    "NOTE: The term \"layer\" most commonly refers to the inputs or outputs of the weight matrix or activations functions and not the linear layer or activation layer themselves. Output layers in between two \"linear layers\" are called \"hidden layers\". You can imagine them \"inside\" the neural network with us only being able to see the input and output layers. To confuse things even further the outputs of activation functions are also commonly called \"activations\"\n",
    "\n",
    "Why do we want a linear function approximator? Because many processes, tasks, systems in the real world are non-linear. \"Linear\" in basic terms refers to any process that takes inputs, scales them and sums them together to get an output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hand Build a Non-Linear Neural Network!</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the equation 0 = w0*x0 + w1*x1 + w2\n",
    "# Consider each input dimention intependantly \n",
    "# Therefore we have 2 equations 0 = w0*x0 + w1\n",
    "# -w1/w0 = 1.5\n",
    "# Pick any w1 and calculate w0\n",
    "\n",
    "w0 = 1\n",
    "w1 = -(1.5 * w0)\n",
    "w_0 = torch.FloatTensor([[w0, 0], [0, w0]])\n",
    "b_0 = torch.FloatTensor([[w1, w1]])\n",
    "print(\"Weights\", w_0.numpy())\n",
    "print(\"bias\", b_0.numpy())\n",
    "\n",
    "h1 = F.linear(x_train, w_0, bias=b_0)\n",
    "h2 = torch.sigmoid(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out the Raw data vs Raw output and Sigmoid output\n",
    "# What happens as you increase w0???\n",
    "# Increase w0 untill the data becomes linearly seperable!\n",
    "\n",
    "plt.figure(figsize=(17, 5))\n",
    "plt.subplot(131)\n",
    "plt.scatter(x_train[:, 0].numpy(), x_train[:, 1].numpy(), c=y_train.flatten().numpy())\n",
    "plt.title(\"Raw Data\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(h1[:, 0].numpy(), h1[:, 1].numpy(), c=y_train.flatten().numpy())\n",
    "plt.title(\"Raw output\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(h2[:, 0].numpy(), h2[:, 1].numpy(), c=y_train.flatten().numpy())\n",
    "plt.title(\"Sigmoid output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Second Layer</h3>\n",
    "Now that we've mapped the input data into a space that is <b> linearly separable </b> we can create ANOTHER layer that takes in this second (hidden) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line between the points (0, 0.6) and (0.6, 0)\n",
    "point0 = torch.tensor([0.6, 0])\n",
    "point1 = torch.tensor([0, 0.6])\n",
    "\n",
    "w0 = 1\n",
    "w1 = 1\n",
    "w2 = -0.6\n",
    "w_1 = torch.FloatTensor([[w0, w1]])\n",
    "b_1 = torch.FloatTensor([w2])\n",
    "print(\"Weights\", w_1.numpy())\n",
    "print(\"bias\", b_1.numpy())\n",
    "\n",
    "h3 = F.linear(h2, w_1, bias=b_1)\n",
    "class_pred = (h3 > 0).float()\n",
    "\n",
    "# Calculate the accuracy!\n",
    "acc = float(sum(class_pred == y_train))/ float(y_train.shape[0])\n",
    "print(\"Non-linear accuracy %.2f%%\" % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "plt.scatter(x_train[:, 0].numpy(), x_train[:, 1].numpy(), c=class_pred.flatten().numpy())\n",
    "_ = plt.xlabel(\"x0\")\n",
    "_ = plt.ylabel(\"x1\")\n",
    "_ = plt.title(\"Non-Linear Prediction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train a Non-Linear Logistic Regression Model With Pytorch</h2>\n",
    "Now that we have seen the power of a non-linear Neural Network let's see how we can create and train one automatically with Pytorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the non-linear architecture that we used with Pytorch linear layers!\n",
    "class NonLinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(NonLinearModel, self).__init__() \n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)         \n",
    "        self.linear2 = nn.Linear(hidden_size, output_size) \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h1 = self.linear1(x)\n",
    "        self.h2 = torch.sigmoid(self.h1)\n",
    "        self.h3 = self.linear2(self.h2)\n",
    "        \n",
    "        return self.h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameters of the Neural Network are randomly initialised so we must use gradient decent to find the \n",
    "# optimal parameters!\n",
    "\n",
    "logistic_nonlinear = NonLinearModel(input_size=2, output_size=1, hidden_size=2) \n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create our optimizer - lr = 0.1\n",
    "logistic_optimizer = torch.optim.Adam(logistic_nonlinear.parameters(), lr=1e-2)\n",
    "\n",
    "logistic_loss_log = [] # keep track of the loss values\n",
    "logistic_acc = [] # keep track of the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times we iterate over the dataset\n",
    "max_epoch = 5000\n",
    "\n",
    "for epoch in trange(max_epoch, desc=\"Training Epochs\"):\n",
    "    with torch.no_grad():\n",
    "        y_test_hat = logistic_nonlinear(x_test)\n",
    "        \n",
    "        # The descision boundary is at 0.5 (between 0 and 1) AFTER the sigmoid\n",
    "        # The input to the Sigmoid function that gives 0.5 is 0!\n",
    "        # Therefore the descision boundary for the RAW output is at 0!!\n",
    "        class_pred = (y_test_hat > 0).float()\n",
    "        logistic_acc.append(float(sum(class_pred == y_test))/ float(y_test.shape[0]))\n",
    "        \n",
    "    # Perform a training step\n",
    "    y_train_hat = logistic_nonlinear(x_train)\n",
    "    loss = loss_function(y_train_hat, y_train)\n",
    "    \n",
    "    logistic_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    logistic_optimizer.step()\n",
    "\n",
    "    logistic_loss_log.append(loss.item())\n",
    "    \n",
    "print(\"Accuracy of Non-linear model(GD): %.2f%% \" %(logistic_acc[-1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logistic_acc)\n",
    "_ = plt.xlabel(\"Iterations\")\n",
    "_ = plt.ylabel(\"Accuracy\")\n",
    "_ = plt.title(\"Test Accuracy During training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logistic_loss_log)\n",
    "_ = plt.xlabel(\"Iterations\")\n",
    "_ = plt.ylabel(\"Loss\")\n",
    "_ = plt.title(\"Training Loss During training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_test_hat = logistic_nonlinear(x_test)\n",
    "    class_pred = (y_test_hat > 0).float()\n",
    "    \n",
    "plt.figure(figsize=(5, 5))    \n",
    "plt.scatter(x_test[:, 0].numpy(), x_test[:, 1].numpy(), c=class_pred.flatten().numpy())\n",
    "_ = plt.xlabel(\"x0\")\n",
    "_ = plt.ylabel(\"x1\")\n",
    "_ = plt.title(\"Non-Linear Logistic Prediction!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(logistic_nonlinear.h2[:, 0].numpy(), \n",
    "            logistic_nonlinear.h2[:, 1].numpy(), \n",
    "            c=class_pred.flatten().numpy())\n",
    "\n",
    "_ = plt.xlabel(\"h2_0\")\n",
    "_ = plt.ylabel(\"h2_1\")\n",
    "_ = plt.title(\"h2 Hidden Layer Vs Prediction!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_nonlinear.linear2.bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
