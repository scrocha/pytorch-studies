{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2340/1*Fw-ehcNBR9byHtho-Rxbtw.gif\" width=\"750\" align=\"center\">\n",
    "The 2D Image convolution is a fairly simple operation that leads to powerful and somewhat surprising results! In this notebook we'll look at performing convolutions with a hand-crafted kernal and then look at how we can learn the parameters of a kernal to perform some task!\n",
    "\n",
    "Have a look at this interactive convolution visualiser\n",
    "[Convolution Visualizer](https://ezyang.github.io/convolution-visualizer/index.html) <br>\n",
    "[Youtube: But what is a convolution? by 3Blue1Brown](https://youtu.be/KuXjwB4LzSA?si=G3Gb-GB5XAvwuw44) <br>\n",
    "[Youtube:  Convolutions in image processing | Week 1 | MIT 18.S191 Fall 2020 | Grant Sanderson ](https://www.youtube.com/live/8rrHTtUzyZA?si=poQ-lPjJa6YeoJLu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a test image to experiment with using the Python Imaging Library (PIL).<br>\n",
    "Note: PIL images are thenselves objects and the image can be displayed just by printing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img1 = Image.open(\"../data/puppy.jpg\").convert('RGB')\n",
    "test_img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a Pytorch \"transform\" using the torchvision library \n",
    "# This particular transform simply takes a PIL image and converts it to a tensor\n",
    "transform = T.ToTensor()\n",
    "resize = T.Resize(512)\n",
    "test_img1 = transform(resize(test_img1))\n",
    "print(\"Image Shape: \", test_img1.shape)\n",
    "# NOTE:Many torchvision functions with only work on PIL images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hand Crafted Convolution Kernels</h2>\n",
    "\n",
    "<h3>Sobel Edge Detector</h3>\n",
    "The <a href=\"https://en.wikipedia.org/wiki/Sobel_operator\">Sobel Edge detector</a> is a famous and simple convolutional kernal filter that will \"extract\" the edges of an image and was/is extensively used as a part of many algorithms. Here we will create a Sobel Filter and use it on our test image.<br> By looking at the filter values can you tell how it works?<br><br>\n",
    "Convolution with a Sobel Kernel (left) and the features extracted by a Sobel edge detector (right)<br>\n",
    "<img src=\"https://miro.medium.com/max/1356/1*-OM6jQTMNACDX2vAh_lvMQ.png\" width=\"480\" align=\"left\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/17/Bikesgraysobel.jpg\" width=\"480\" align=\"right\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we must create a filter that will extract edges in the X direction\n",
    "gx = torch.FloatTensor([[-1, 0, 1],[-2, 0, 2],[-1, 0, 1]]).unsqueeze(0)\n",
    "gx = torch.repeat_interleave(gx, 3, 0).unsqueeze(0)\n",
    "print(\"Kernel Shape: \", gx.shape)\n",
    "print(gx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we must create a filter that will extract edges in the Y direction\n",
    "gy = torch.FloatTensor([[1, 2, 1],[0, 0, 0],[-1, -2, -1]]).unsqueeze(0)\n",
    "gy = torch.repeat_interleave(gy, 3, 0).unsqueeze(0)\n",
    "print(\"Kernel Shape: \", gy.shape)\n",
    "print(gy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Convolutions in Pytorch </h3>\n",
    "\n",
    "For the function Conv2d (which performs the convolution operation) the convolutional kernel must be of the shape  <br>\n",
    "<b>[out channels, in channels, kernel height, kernel width]</b> <br>\n",
    "The input image must have the shape <br>\n",
    "<b>[minibatch size, in channels, image height, image width]</b> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolve the image with the X direction kernel\n",
    "conv_out1 = F.conv2d(test_img1.unsqueeze(0), gx, bias=None, stride=1)\n",
    "print(\"Feature Map Shape: \", conv_out1.shape)\n",
    "\n",
    "_ = plt.imshow(torch.squeeze(conv_out1), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolve the image with the Y direction kernel\n",
    "conv_out2 = F.conv2d(test_img1.unsqueeze(0), gy, bias=None, stride=1)\n",
    "print(\"Feature Map Shape: \", conv_out2.shape)\n",
    "\n",
    "_ = plt.imshow(torch.squeeze(conv_out2), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two resulting images together by finding the magnitude\n",
    "conv_out = (conv_out1.pow(2) + conv_out2.pow(2)).sqrt()\n",
    "plt.figure(figsize = (10,10))\n",
    "_ = plt.imshow(conv_out.squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Convolutional layers </h3>\n",
    "Instead of doing each convolution separately, we can concatenate the kernels and perform the convolution with both kernels in one step! The ouput will have the 2D feature maps from each kernel concatenated along the channel dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate both kernels \n",
    "g_xy = torch.cat((gx, gy), 0)\n",
    "\n",
    "# We can perform the convolutions of both kernels in the same step\n",
    "conv_out3 = F.conv2d(test_img1.unsqueeze(0), g_xy, bias=None, stride=1)\n",
    "\n",
    "# We can now easily find the magnitude\n",
    "conv_out_mag = conv_out3.pow(2).sum(1).sqrt()\n",
    "\n",
    "# We should get the same output as before!\n",
    "plt.figure(figsize = (10,10))\n",
    "_ = plt.imshow(conv_out_mag.squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope! They still look the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Train a Convolution Kernel! </h2>\n",
    "So how do you look for something when you don't know what you are looking for? Often in computer vision we know WHAT we want, but we aren't sure how to get it! For example if you want to create an algorithm to classify cats, you might start by creating a list of all the uniquely cat-like things about an image of a cat, but then how do you go about extracting them from an image?? What if there's something you forgot? <br>\n",
    "So in comes machine learning, we specify an objective (a cost function or a loss) for some learning model with the hope that by minimizing that loss our model will do what we want! (as you may have learnt by now there's a LOT more to it then that)<br>.\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-b662a8fc3be57f76c708c171fcf29960\" width=\"480\" align=\"center\">\n",
    "\n",
    "Did you know that Convolutions are implemented as a <a href=\"https://www.baeldung.com/cs/convolution-matrix-multiplication\">Matrix Multiplication?</a> By doing so they are far easier to implement and can easily utilise harware accelerators to speed them up!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html\n",
    "# torch.nn.Conv2d\n",
    "\n",
    "# Lets create a learnable 2D convolutional layer\n",
    "# in_channels  - the number of input channels\n",
    "# out_channels - the number of output channels - also the number of kernels in a layer\n",
    "# kernel_size  - the height and width of our kernel - can specify with a tuple for non-square kernels\n",
    "# stride       - the number of pixels the kernel will \"step\"\n",
    "# bias         - same as a linear layer, 1 bias term per output channel\n",
    "\n",
    "conv_kernel = nn.Conv2d(in_channels = 3, out_channels = 1, \n",
    "                        kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = optim.SGD(conv_kernel.parameters(), lr = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the kernel's shape\n",
    "conv_kernel.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll pass both images through the randomly initialised convolutional layers\n",
    "with torch.no_grad():\n",
    "    imgs_out = conv_kernel(test_img1.unsqueeze(0))\n",
    "\n",
    "_ = plt.imshow(imgs_out[0, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a target for our Convolution output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur = T.GaussianBlur(7, 4)\n",
    "# Blur the image and subtract this from the original\n",
    "# Blurring will leave only the high frequency components on the image\n",
    "# By subtracting them from the original we should get only the high frequency components\n",
    "target = test_img1.unsqueeze(0) - blur(test_img1.unsqueeze(0))\n",
    "_ = plt.imshow(torch.squeeze(target[0, 0]), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training our kernel!</b><br>\n",
    "Have you come up with a loss function to use yet? Perform GD with it and see what you get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = []\n",
    "\n",
    "for _ in trange(2000):\n",
    "    imgs_out = conv_kernel(test_img1.unsqueeze(0))\n",
    "\n",
    "    loss = (target - imgs_out).pow(2).mean()\n",
    "    logger.append(loss.item())\n",
    "    conv_kernel.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logger[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize = (20,10))\n",
    "ax2.imshow(torch.squeeze(imgs_out[0, 0].detach()), cmap='gray')\n",
    "ax1.imshow(torch.squeeze(target[0, 0]), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(conv_kernel.weight.data[0].mean(0).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
